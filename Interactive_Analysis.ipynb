{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Loan Approval Dataset - Interactive Analysis\n",
    "\n",
    "This notebook provides an interactive environment for exploring the loan approval dataset.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Load and explore data using pandas\n",
    "- Visualize distributions and relationships\n",
    "- Understand data cleaning techniques\n",
    "- Prepare data for machine learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup and Import Libraries\n",
    "\n",
    "First, let's import all the libraries we'll need for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import normaltest, shapiro\n",
    "\n",
    "# Display settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make plots appear in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load the Dataset\n",
    "\n",
    "Let's load our loan approval dataset. You can use either:\n",
    "- `sample_loan_data.csv` (provided sample)\n",
    "- `loan_approval.csv` (your own dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Update the filename if using a different file\n",
    "df = pd.read_csv('../data/sample_loan_data.csv')\n",
    "\n",
    "print(f\"‚úì Dataset loaded successfully!\")\n",
    "print(f\"  Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Initial Data Exploration\n",
    "\n",
    "Let's take a first look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset information\n",
    "print(\"Dataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary for numerical columns\n",
    "print(\"Statistical Summary (Numerical Columns):\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Missing Values Analysis\n",
    "\n",
    "Identifying and understanding missing values is crucial for data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing_Count': missing_values.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "# Show only columns with missing values\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"Columns with Missing Values:\")\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"‚úì No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "if len(missing_df) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(missing_df['Column'], missing_df['Missing_Percentage'], color='salmon')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Missing Percentage (%)')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Distribution Analysis\n",
    "\n",
    "Understanding the distribution of features helps us choose appropriate analysis techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for all numerical features\n",
    "if len(numerical_cols) > 0:\n",
    "    fig, axes = plt.subplots(len(numerical_cols), 2, figsize=(15, 5 * len(numerical_cols)))\n",
    "    \n",
    "    for idx, col in enumerate(numerical_cols):\n",
    "        # Remove missing values\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        # Histogram\n",
    "        axes[idx, 0].hist(data, bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[idx, 0].set_title(f'{col} - Histogram')\n",
    "        axes[idx, 0].set_xlabel(col)\n",
    "        axes[idx, 0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Box plot\n",
    "        axes[idx, 1].boxplot(data, vert=True)\n",
    "        axes[idx, 1].set_title(f'{col} - Box Plot')\n",
    "        axes[idx, 1].set_ylabel(col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality tests for numerical columns\n",
    "print(\"Normality Tests (Shapiro-Wilk):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for col in numerical_cols:\n",
    "    data = df[col].dropna()\n",
    "    if len(data) > 3:  # Need at least 3 samples\n",
    "        stat, p_value = shapiro(data)\n",
    "        is_normal = \"‚úì Normal\" if p_value > 0.05 else \"‚úó Not Normal\"\n",
    "        print(f\"{col:20s} p-value: {p_value:.4f}  ‚Üí  {is_normal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Categorical Variables Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar charts for categorical variables\n",
    "if len(categorical_cols) > 0:\n",
    "    fig, axes = plt.subplots((len(categorical_cols) + 1) // 2, 2, \n",
    "                             figsize=(15, 4 * ((len(categorical_cols) + 1) // 2)))\n",
    "    axes = axes.flatten() if len(categorical_cols) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(categorical_cols):\n",
    "        value_counts = df[col].value_counts()\n",
    "        axes[idx].bar(value_counts.index, value_counts.values, color='steelblue')\n",
    "        axes[idx].set_title(f'{col} - Distribution')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Count')\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for i, v in enumerate(value_counts.values):\n",
    "            axes[idx].text(i, v, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(categorical_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Correlation Analysis\n",
    "\n",
    "Understanding relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for numerical features\n",
    "if len(numerical_cols) > 1:\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=1, fmt='.2f')\n",
    "    plt.title('Correlation Matrix - Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nStrong correlations (|r| > 0.5):\")\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.5:\n",
    "                print(f\"  {correlation_matrix.columns[i]} ‚Üî {correlation_matrix.columns[j]}: \"\n",
    "                      f\"{correlation_matrix.iloc[i, j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Target Variable Analysis\n",
    "\n",
    "If your dataset has a target variable (e.g., `Loan_Status`), analyze it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Loan_Status exists\n",
    "if 'Loan_Status' in df.columns:\n",
    "    target_counts = df['Loan_Status'].value_counts()\n",
    "    target_pct = df['Loan_Status'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(\"Loan Status Distribution:\")\n",
    "    print(f\"  Approved (Y): {target_counts.get('Y', 0)} ({target_pct.get('Y', 0):.1f}%)\")\n",
    "    print(f\"  Rejected (N): {target_counts.get('N', 0)} ({target_pct.get('N', 0):.1f}%)\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Bar chart\n",
    "    axes[0].bar(target_counts.index, target_counts.values, color=['green', 'red'])\n",
    "    axes[0].set_title('Loan Status - Bar Chart')\n",
    "    axes[0].set_xlabel('Status')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%',\n",
    "                colors=['green', 'red'], startangle=90)\n",
    "    axes[1].set_title('Loan Status - Pie Chart')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Loan_Status column not found in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Your Turn - Exploratory Questions\n",
    "\n",
    "Try answering these questions using code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: What is the average income of applicants?\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: What percentage of applicants are married?\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: Is there a relationship between education and loan approval?\n",
    "# Hint: Use pd.crosstab() or groupby()\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "- ‚úì Load and explore datasets\n",
    "- ‚úì Identify and visualize missing values\n",
    "- ‚úì Analyze distributions (numerical and categorical)\n",
    "- ‚úì Test for normality\n",
    "- ‚úì Explore correlations\n",
    "- ‚úì Analyze target variables\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run the cleaning script (`3_data_cleaning.py`) to prepare the data\n",
    "2. Experiment with feature engineering\n",
    "3. Build predictive models\n",
    "\n",
    "---\n",
    "**Happy Learning! üìö**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
